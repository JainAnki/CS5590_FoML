# -*- coding: utf-8 -*-
"""4a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gk2-WTyggn4svY6i0MibNtZC1IqYqCD3
"""

from sklearn.model_selection import train_test_split
import numpy as np
from csv import reader
import random
from pandas import DataFrame as df
import sklearn.ensemble
from sklearn.metrics import accuracy_score
from randomforest import RandomForestClassifier
import time

# Convert string column to float
def str_column_to_float(dataset, column):
    for row in dataset:
        row[column] = float(row[column].strip())
# Convert string column to integer
def str_column_to_int(dataset, column):
    class_values = [row[column] for row in dataset]
    unique = set(class_values)
    lookup = dict()
    for i, value in enumerate(unique):
        lookup[value] = i
    for row in dataset:
        row[column] = lookup[row[column]]
    return lookup

with open("/content/spam.data.txt",'r') as f:
            plaintext = f.read()
plaintext = plaintext.replace(' ',',')    
with open("spam.data.csv",'w') as f:
    f.write(plaintext)

dataset = list()
with open("spam.data.csv", 'r') as file:
    csv_reader = reader(file)
    for row in csv_reader:
        if not row:
            continue
        dataset.append(row)
    print ("Number of records: %d" % len(dataset))
    random.shuffle(dataset)
# convert string attributes to integers
for i in range(0, len(dataset[0])-1):
    str_column_to_float(dataset, i)
# convert class column to integers
str_column_to_int(dataset, len(dataset[0])-1)
df1 = df(dataset)
y1 = df1[57]
del df1[57]
X1  = df1
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1, test_size=0.3, random_state=1234)
X_train = X_train1.to_numpy()
X_test = X_test1.to_numpy()  
y_train = y_train1.to_numpy()
y_test = y_test1.to_numpy()

fo = open('Code_output.txt','w')

print("Number of features used for both kinds of training:")
tic = time.perf_counter()
forest = RandomForestClassifier(max_features = 42)
forest.fit(X_train, y_train)
accuracy = forest.score(X_test, y_test)
toc = time.perf_counter()
print("\n")
print("FINISHED classifying using the code written from scratch. accuracy score : ")
print(100*accuracy)

time_format = time.strftime("Time: %H:%M:%S", time.gmtime(toc-tic))
print("Time taken for the output to be evaluated = ", time_format)
fo.write ('Code from scratch'+'\n'+'The accuracy was '+str( 100*accuracy)+ '% on the test data with number of features = 42'+'\n' + 'Time taken = '+ str(time_format))

print('\n')
tic = time.perf_counter()
model = sklearn.ensemble.RandomForestClassifier(n_estimators = 50, criterion = 'entropy',max_depth = 25, min_samples_split = 42)
model.fit(X_train, y_train)
predicted_labels = model.predict(X_test)
toc = time.perf_counter()
print("FINISHED classifying using scikit-learn's builtin library. accuracy score : ")
accuracy= accuracy_score(y_test, predicted_labels)
print(100*accuracy)

print("Time taken for the output to be evaluated = ", (toc-tic), "seconds")
fo.write ('\n'+'\n'+'Built-in scikit-learn library'+'\n'+'The accuracy was '+str( 100*accuracy)+ '% on the test data with number of features = 42' +'\n' + 'Time taken = '+ str(toc-tic) + ' seconds')

fo.close()

"""##Inference

The **processing** **time** of the **scikit-learn built-in library** is much **reduced** while the **accuracy** of the **code from scratch** is slightly **higher**.
"""